{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import gensim.models.word2vec as w2v\n",
    "import string\n",
    "\n",
    "#nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import map_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#sklearn\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#stats\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "#metrics\n",
    "import mlc_metrics as metrics\n",
    "\n",
    "#Define Scorer for Cross-Validation\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    return jaccard_similarity_score(ground_truth,predictions)\n",
    "def my_hammloss(ground_truth, predictions):\n",
    "    return metrics.mlc_hamming_loss(ground_truth, predictions)\n",
    "def my_custom_f1(ground_truth, predictions):\n",
    "    return metrics.mlc_f1score(ground_truth,predictions)\n",
    "\n",
    "jaccard  = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "hammloss = make_scorer(my_hammloss)\n",
    "f1score = make_scorer(my_custom_f1,greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "def collapse_4andabove(stringlabels):\n",
    "    arraylabels = []\n",
    "    for i in stringlabels.strip().split(','):\n",
    "        if len(i) != 0:\n",
    "            label = int(i.strip())\n",
    "            if label == 1 or label == 2 or label == 3:\n",
    "                arraylabels.append(label)\n",
    "            else:\n",
    "                label = 4\n",
    "                if label not in arraylabels:\n",
    "                    arraylabels.append(label)\n",
    "        else:\n",
    "            print(\"Found data not annotated\")\n",
    "    return arraylabels\n",
    "\n",
    "# Load a multi-label dataset\n",
    "with open('datafinal/fulldata.csv', encoding='utf-8') as csvfile:\n",
    "    next(csvfile, None)\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "    n = 0\n",
    "    for row in csvreader:\n",
    "        sentence = row[2].lower()\n",
    "        labels = collapse_4andabove(row[3])\n",
    "        X.append(sentence)\n",
    "        Y.append(labels)\n",
    "        n += 1\n",
    "        \n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7198, 7198)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X),n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "additional_stopwords = [\"im\",\"weve\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(additional_stopwords)#add another stop words\n",
    "stop_words = stop_words.union(list(string.punctuation))#add punctuation\n",
    "\n",
    "def stopword_and_punc_removal(x):\n",
    "    return \" \".join(filter(lambda word: word not in stop_words, re.sub(\"[^a-zA-z]\",\" \",x).split()))\n",
    "#     return x\n",
    "\n",
    "def tag_pos(x):\n",
    "#     token = TweetTokenizer().tokenize(x)\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",x)\n",
    "    token = clean.split()\n",
    "    pos = pos_tag(token)\n",
    "    simplified_tags = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in pos]\n",
    "    return simplified_tags\n",
    "\n",
    "def tokenize(x):\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",x)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# print(\"Loading Amazon pre-trained Word2Vec\")\n",
    "# # Amazon_w2v = w2v.Word2Vec.load(os.path.join(\"AmazonW2VtrainedLowerNew\",\"AmazonW2VtrainedLowerNew.w2v\"))\n",
    "# Amazon_w2v = w2v.Word2Vec.load(os.path.join(\"word2vec/NotIncludeDataset\",\"NotIncludeDataset.w2v\"))\n",
    "# print(\"Word2Vec Loaded!\")\n",
    "\n",
    "def generate_embedding(w2vname,X,stopword=False,verbose=False):\n",
    "    print(\"Loading Amazon pre-trained Word2Vec:\",w2vname)\n",
    "    # Amazon_w2v = w2v.Word2Vec.load(os.path.join(\"AmazonW2VtrainedLowerNew\",\"AmazonW2VtrainedLowerNew.w2v\"))\n",
    "    path = 'word2vec/'+w2vname\n",
    "    filename = w2vname+\".w2v\"\n",
    "    w2vmodel = w2v.Word2Vec.load(os.path.join(path,filename))\n",
    "    print(\"Word2Vec Loaded!\")\n",
    "    pos_filter = ['NOUN','VERB','ADV','ADJ']\n",
    "    x_embedding = []\n",
    "    N = len(X)\n",
    "    percent_done = 0\n",
    "    count_exist = 0\n",
    "    count_doesnt_exist = 0\n",
    "    for i in range(N):\n",
    "        count = 1\n",
    "        sent_vector = np.zeros(300) #initialize a dummy vector\n",
    "        if(stopword):\n",
    "            sent_tagged_pos = tag_pos(stopword_and_punc_removal(X[i]))\n",
    "        else:\n",
    "            sent_tagged_pos = tag_pos(X[i])\n",
    "#         print(sent_tagged_pos)\n",
    "        for word in sent_tagged_pos:\n",
    "            if(word[1] in pos_filter):\n",
    "                if(word[0] in w2vmodel.wv.vocab):\n",
    "                    sent_vector += w2vmodel[word[0]]\n",
    "                    count+=1\n",
    "                    count_exist +=1\n",
    "                else:\n",
    "                    #try to make it lowercase- if word2vec not lower\n",
    "                    if(word[0].lower() in w2vmodel.wv.vocab):\n",
    "                        sent_vector += w2vmodel[word[0].lower()]\n",
    "                        count+=1\n",
    "                        count_exist+=1\n",
    "                    else:\n",
    "                        count_doesnt_exist+=1\n",
    "                        \n",
    "        #averaging vector\n",
    "        sent_vector /= count\n",
    "    \n",
    "        if(np.isnan(np.min(sent_vector))):\n",
    "            print(\"YES\")\n",
    "            continue\n",
    "        \n",
    "        x_embedding.append(sent_vector)\n",
    "        \n",
    "        if(verbose):\n",
    "            percent_done += 1\n",
    "            if(percent_done % int(0.25*N)) == 0:\n",
    "                print(\"Progress: \",percent_done,\" / \",N)\n",
    "    if(verbose):\n",
    "        print(\"Done!\")\n",
    "        print(\"Words exist in W2V: \",count_exist)\n",
    "        print(\"Words don't exist in W2V: \",count_doesnt_exist)\n",
    "    print(len(x_embedding))\n",
    "    return x_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cross_val_score(clf_pipeline,word2vec=False):\n",
    "    Result = {'scores_HL':[],'scores_Acc':[],'scores_EMR':[],'scores_F1':[],'scores_PM':[],'scores_RM':[]}\n",
    "    for i in range(10):\n",
    "        print(\"Analzying Fold:\",i+1)\n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "        file_train_name = 'datafinal/10fold/Fold_train'+str(i+1)+'.csv'\n",
    "        file_test_name = 'datafinal/10fold/Fold_test'+str(i+1)+'.csv'\n",
    "        print(\"Loading for train:\",file_train_name)\n",
    "        #load train fold\n",
    "        with open(file_train_name, newline='', encoding='utf-8') as csvfile:\n",
    "            #next(csvfile, None) # skip first header line\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            for row in csvreader:\n",
    "                X_train.append(row[2].lower())\n",
    "                Y_train.append(collapse_4andabove(row[3]))\n",
    "        #load test fold\n",
    "        print(\"Loading for test:\",file_test_name)\n",
    "        with open(file_test_name, newline='', encoding='utf-8') as csvfile:\n",
    "            #next(csvfile, None) # skip first header line\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            for row in csvreader:\n",
    "                X_test.append(row[2].lower())\n",
    "                Y_test.append(collapse_4andabove(row[3]))\n",
    "        if(word2vec):\n",
    "#             w2vname_fold = 'NotIncludeDataset'\n",
    "            w2vname = 'AmazonW2VtrainedLowerNew'\n",
    "            X_train = generate_embedding(w2vname,X_train,stopword=False,verbose=False)\n",
    "            X_test = generate_embedding(w2vname,X_test,stopword=False,verbose=False)\n",
    "            \n",
    "        Y_train = mlb.transform(Y_train)\n",
    "        Y_test = mlb.transform(Y_test)\n",
    "        clf_pipeline.fit(X_train,Y_train)\n",
    "        y_predict = clf_pipeline.predict(X_test)\n",
    "        #specify the result folder\n",
    "        if(word2vec):\n",
    "            metrics.writeall(Y_test,y_predict,'Results/10foldTop/w2v/Fold_'+str(i+1))\n",
    "        else:\n",
    "            metrics.writeall(Y_test,y_predict,'Results/10foldTop/tfidf/Fold_'+str(i+1))\n",
    "        #------------------------------\n",
    "        Result['scores_HL'].append(metrics.mlc_hamming_loss(Y_test, y_predict))\n",
    "        Result['scores_Acc'].append(metrics.mlc_accuracy_score(Y_test, y_predict))\n",
    "        Result['scores_EMR'].append(accuracy_score(Y_test, y_predict))\n",
    "        Result['scores_F1'].append(metrics.mlc_f1score(Y_test, y_predict))\n",
    "        Result['scores_PM'].append(precision_score(Y_test, y_predict,average='micro'))\n",
    "        Result['scores_RM'].append(recall_score(Y_test, y_predict,average='micro'))\n",
    "    print(\"==\"*30)\n",
    "    print(\"Result:\"+str(Result))\n",
    "    print(\"==\"*30)\n",
    "    print(\"Hamm Loss: %0.3f (+/- %0.3f)\" % (mean(Result['scores_HL']), stdev(Result['scores_HL']) * 2))\n",
    "    print(\"Accuracy: %0.3f (+/- %0.3f)\" % (mean(Result['scores_Acc']), stdev(Result['scores_Acc']) * 2))\n",
    "    print(\"ExactMatchRatio: %0.3f (+/- %0.3f)\" % (mean(Result['scores_EMR']), stdev(Result['scores_EMR']) * 2))\n",
    "    print(\"F1: %0.3f (+/- %0.3f)\" % (mean(Result['scores_F1']), stdev(Result['scores_F1']) * 2))\n",
    "    print(\"Precision Micro: %0.3f (+/- %0.3f)\" % (mean(Result['scores_PM']), stdev(Result['scores_PM']) * 2))\n",
    "    print(\"Recall Micro: %0.3f (+/- %0.3f)\" % (mean(Result['scores_RM']), stdev(Result['scores_RM']) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with TF-idf using LinearSVC\n",
    "Best for jaccard and F1 -> C=0.1, class_weight=\"balanced\"\n",
    "<br>\n",
    "Best for subset accuracy (exact matching) -> C = 1\n",
    "<br>\n",
    "Including stopwords worsen the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for normal text classification with tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,stop_words=None)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', OneVsRestClassifier(LinearSVC(C=1)))\n",
    "#                      ('clf', OneVsRestClassifier(LinearSVC(C=0.1,class_weight='balanced')))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analzying Fold: 1\n",
      "Loading for train: datafinal/10fold/Fold_train1.csv\n",
      "Loading for test: datafinal/10fold/Fold_test1.csv\n",
      "Analzying Fold: 2\n",
      "Loading for train: datafinal/10fold/Fold_train2.csv\n",
      "Loading for test: datafinal/10fold/Fold_test2.csv\n",
      "Analzying Fold: 3\n",
      "Loading for train: datafinal/10fold/Fold_train3.csv\n",
      "Loading for test: datafinal/10fold/Fold_test3.csv\n",
      "Analzying Fold: 4\n",
      "Loading for train: datafinal/10fold/Fold_train4.csv\n",
      "Loading for test: datafinal/10fold/Fold_test4.csv\n",
      "Analzying Fold: 5\n",
      "Loading for train: datafinal/10fold/Fold_train5.csv\n",
      "Loading for test: datafinal/10fold/Fold_test5.csv\n",
      "Analzying Fold: 6\n",
      "Loading for train: datafinal/10fold/Fold_train6.csv\n",
      "Loading for test: datafinal/10fold/Fold_test6.csv\n",
      "Analzying Fold: 7\n",
      "Loading for train: datafinal/10fold/Fold_train7.csv\n",
      "Loading for test: datafinal/10fold/Fold_test7.csv\n",
      "Analzying Fold: 8\n",
      "Loading for train: datafinal/10fold/Fold_train8.csv\n",
      "Loading for test: datafinal/10fold/Fold_test8.csv\n",
      "Analzying Fold: 9\n",
      "Loading for train: datafinal/10fold/Fold_train9.csv\n",
      "Loading for test: datafinal/10fold/Fold_test9.csv\n",
      "Analzying Fold: 10\n",
      "Loading for train: datafinal/10fold/Fold_train10.csv\n",
      "Loading for test: datafinal/10fold/Fold_test10.csv\n",
      "============================================================\n",
      "Result:{'scores_HL': [0.19027777777777777, 0.2107638888888889, 0.21284722222222222, 0.21805555555555556, 0.20590277777777777, 0.2048611111111111, 0.21319444444444444, 0.2, 0.20584144645340752, 0.2148817802503477], 'scores_Acc': [0.5900462962962962, 0.5666666666666665, 0.5553240740740739, 0.5304398148148147, 0.5652777777777775, 0.5783564814814812, 0.5534722222222223, 0.5686342592592593, 0.5603847936949465, 0.54346314325452], 'scores_EMR': [0.44861111111111113, 0.43333333333333335, 0.41805555555555557, 0.40555555555555556, 0.43194444444444446, 0.42777777777777776, 0.41944444444444445, 0.43472222222222223, 0.44089012517385257, 0.40751043115438107], 'scores_F1': [0.66056518946692355, 0.63259408328929001, 0.62496905006858705, 0.59397038768751209, 0.63248948042444197, 0.65618657174680639, 0.62152868284408036, 0.63494956859885776, 0.62042186059212623, 0.61113962600211746], 'scores_PM': [0.71446229913473425, 0.69038701622971288, 0.69974226804123707, 0.67588932806324109, 0.68695652173913047, 0.69987546699875469, 0.70280612244897955, 0.71483870967741936, 0.69731800766283525, 0.69708994708994709], 'scores_RM': [0.64581005586592177, 0.60635964912280704, 0.5882990249187432, 0.57318435754189945, 0.61856823266219241, 0.61690450054884738, 0.59120171673819744, 0.60946094609460943, 0.60599334073251943, 0.5753275109170306]}\n",
      "============================================================\n",
      "Hamm Loss: 0.208 (+/- 0.016)\n",
      "Accuracy: 0.561 (+/- 0.034)\n",
      "ExactMatchRatio: 0.427 (+/- 0.028)\n",
      "F1: 0.629 (+/- 0.039)\n",
      "Precision Micro: 0.698 (+/- 0.024)\n",
      "Recall Micro: 0.603 (+/- 0.044)\n"
     ]
    }
   ],
   "source": [
    "get_cross_val_score(text_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('clf',OneVsRestClassifier(LinearSVC(C=1)))\n",
    "#     ('clf',OneVsRestClassifier(LinearSVC(C=0.1,class_weight='balanced')))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analzying Fold: 1\n",
      "Loading for train: datafinal/10fold/Fold_train1.csv\n",
      "Loading for test: datafinal/10fold/Fold_test1.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6478\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "720\n",
      "Analzying Fold: 2\n",
      "Loading for train: datafinal/10fold/Fold_train2.csv\n",
      "Loading for test: datafinal/10fold/Fold_test2.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6478\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "720\n",
      "Analzying Fold: 3\n",
      "Loading for train: datafinal/10fold/Fold_train3.csv\n",
      "Loading for test: datafinal/10fold/Fold_test3.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6478\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "720\n",
      "Analzying Fold: 4\n",
      "Loading for train: datafinal/10fold/Fold_train4.csv\n",
      "Loading for test: datafinal/10fold/Fold_test4.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6478\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "720\n",
      "Analzying Fold: 5\n",
      "Loading for train: datafinal/10fold/Fold_train5.csv\n",
      "Loading for test: datafinal/10fold/Fold_test5.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6478\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "720\n",
      "Analzying Fold: 6\n",
      "Loading for train: datafinal/10fold/Fold_train6.csv\n",
      "Loading for test: datafinal/10fold/Fold_test6.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6478\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "720\n",
      "Analzying Fold: 7\n",
      "Loading for train: datafinal/10fold/Fold_train7.csv\n",
      "Loading for test: datafinal/10fold/Fold_test7.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6478\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "720\n",
      "Analzying Fold: 8\n",
      "Loading for train: datafinal/10fold/Fold_train8.csv\n",
      "Loading for test: datafinal/10fold/Fold_test8.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6478\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "720\n",
      "Analzying Fold: 9\n",
      "Loading for train: datafinal/10fold/Fold_train9.csv\n",
      "Loading for test: datafinal/10fold/Fold_test9.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6479\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "719\n",
      "Analzying Fold: 10\n",
      "Loading for train: datafinal/10fold/Fold_train10.csv\n",
      "Loading for test: datafinal/10fold/Fold_test10.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6479\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "719\n",
      "============================================================\n",
      "Result:{'scores_HL': [0.16215277777777778, 0.19479166666666667, 0.19722222222222222, 0.17708333333333334, 0.19236111111111112, 0.1795138888888889, 0.2076388888888889, 0.18993055555555555, 0.19541029207232266, 0.19019471488178025], 'scores_Acc': [0.6516203703703702, 0.5870370370370371, 0.5857638888888888, 0.6208333333333332, 0.5909722222222223, 0.6259259259259257, 0.5726851851851852, 0.6032407407407406, 0.5839128419100601, 0.6069772832637923], 'scores_EMR': [0.51527777777777772, 0.44027777777777777, 0.45416666666666666, 0.50138888888888888, 0.46527777777777779, 0.49305555555555558, 0.42638888888888887, 0.48055555555555557, 0.46036161335187759, 0.47705146036161333], 'scores_F1': [0.71955867722314548, 0.6604049830587686, 0.65263979183083787, 0.68168749606992385, 0.65280683094014058, 0.69337565828512093, 0.64706914308800934, 0.66396416676348702, 0.64630138236642998, 0.67259863007347298], 'scores_PM': [0.77020202020202022, 0.7218710493046776, 0.73386034255599475, 0.75229357798165142, 0.72077922077922074, 0.75191815856777489, 0.71085858585858586, 0.73941798941798942, 0.72156862745098038, 0.73684210526315785], 'scores_RM': [0.68156424581005581, 0.62609649122807021, 0.60346695557963159, 0.64134078212290502, 0.62080536912751683, 0.64544456641053782, 0.60407725321888417, 0.61496149614961493, 0.61265260821309653, 0.6266375545851528]}\n",
      "============================================================\n",
      "Hamm Loss: 0.189 (+/- 0.025)\n",
      "Accuracy: 0.603 (+/- 0.048)\n",
      "ExactMatchRatio: 0.471 (+/- 0.055)\n",
      "F1: 0.669 (+/- 0.047)\n",
      "Precision Micro: 0.736 (+/- 0.036)\n",
      "Recall Micro: 0.628 (+/- 0.047)\n"
     ]
    }
   ],
   "source": [
    "get_cross_val_score(pipeline,word2vec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
