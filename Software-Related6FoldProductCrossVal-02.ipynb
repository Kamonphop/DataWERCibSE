{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import gensim.models.word2vec as w2v\n",
    "import string\n",
    "\n",
    "#nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import map_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#sklearn\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#statistics\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "#metrics\n",
    "import mlc_metrics as metrics\n",
    "\n",
    "#Define Scorer for Cross-Validation\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    return jaccard_similarity_score(ground_truth,predictions)\n",
    "def my_hammloss(ground_truth, predictions):\n",
    "    return metrics.mlc_hamming_loss(ground_truth, predictions)\n",
    "def my_custom_f1(ground_truth, predictions):\n",
    "    return metrics.mlc_f1score(ground_truth,predictions)\n",
    "\n",
    "jaccard  = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "hammloss = make_scorer(my_hammloss)\n",
    "f1score = make_scorer(my_custom_f1,greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1923\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LabelEncoder' object has no attribute 'inverse_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-994426f32dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallfold_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LabelEncoder' object has no attribute 'inverse_labels'"
     ]
    }
   ],
   "source": [
    "allfold_Y = []\n",
    "with open('datafinal/fulldata.csv', encoding='utf-8') as csvfile:\n",
    "    next(csvfile, None) # skip first header line\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "    n = 0\n",
    "    for row in csvreader:\n",
    "        if( \"2\" in row[3] ):\n",
    "            if(row[5] == 'Inquiry'):\n",
    "                allfold_Y.append('Problem Discovery')\n",
    "            else:\n",
    "                allfold_Y.append(row[5])\n",
    "print(len(allfold_Y))\n",
    "le = LabelEncoder()\n",
    "le.fit(allfold_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "additional_stopwords = [\"im\",\"weve\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(additional_stopwords)#add another stop words\n",
    "stop_words = stop_words.union(list(string.punctuation))#add punctuation\n",
    "def stopword_and_punc_removal(x):\n",
    "    return \" \".join(filter(lambda word: word not in stop_words, re.sub(\"[^a-zA-z]\",\" \",x).split()))\n",
    "#     return x\n",
    "def tag_pos(x):\n",
    "#     token = TweetTokenizer().tokenize(x)\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",x)\n",
    "    token = clean.split()\n",
    "    pos = pos_tag(token)\n",
    "    simplified_tags = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in pos]\n",
    "    return simplified_tags\n",
    "\n",
    "def tokenize(x):\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",x)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def generate_embedding(w2vname,X,stopword=False,verbose=False):\n",
    "    print(\"Loading Amazon pre-trained Word2Vec:\",w2vname)\n",
    "    # Amazon_w2v = w2v.Word2Vec.load(os.path.join(\"AmazonW2VtrainedLowerNew\",\"AmazonW2VtrainedLowerNew.w2v\"))\n",
    "    path = 'word2vec/'+w2vname\n",
    "    filename = w2vname+\".w2v\"\n",
    "    w2vmodel = w2v.Word2Vec.load(os.path.join(path,filename))\n",
    "    print(\"Word2Vec Loaded!\")\n",
    "    pos_filter = ['NOUN','VERB','ADV','ADJ']\n",
    "    x_embedding = []\n",
    "    N = len(X)\n",
    "    percent_done = 0\n",
    "    count_exist = 0\n",
    "    count_doesnt_exist = 0\n",
    "    for i in range(N):\n",
    "        count = 1\n",
    "        sent_vector = np.zeros(300) #initialize a dummy vector\n",
    "        if(stopword):\n",
    "            sent_tagged_pos = tag_pos(stopword_and_punc_removal(X[i]))\n",
    "        else:\n",
    "            sent_tagged_pos = tag_pos(X[i])\n",
    "#         print(sent_tagged_pos)\n",
    "        for word in sent_tagged_pos:\n",
    "            if(word[1] in pos_filter):\n",
    "                if(word[0] in w2vmodel.wv.vocab):\n",
    "                    sent_vector += w2vmodel[word[0]]\n",
    "                    count+=1\n",
    "                    count_exist +=1\n",
    "                else:\n",
    "                    #try to make it lowercase- if word2vec not lower\n",
    "                    if(word[0].lower() in w2vmodel.wv.vocab):\n",
    "                        sent_vector += w2vmodel[word[0].lower()]\n",
    "                        count+=1\n",
    "                        count_exist+=1\n",
    "                    else:\n",
    "                        count_doesnt_exist+=1\n",
    "                        \n",
    "        #averaging vector\n",
    "        sent_vector /= count\n",
    "    \n",
    "        if(np.isnan(np.min(sent_vector))):\n",
    "            print(\"YES\")\n",
    "            continue\n",
    "        \n",
    "        x_embedding.append(sent_vector)\n",
    "        \n",
    "        if(verbose):\n",
    "            percent_done += 1\n",
    "            if(percent_done % int(0.25*N)) == 0:\n",
    "                print(\"Progress: \",percent_done,\" / \",N)\n",
    "    if(verbose):\n",
    "        print(\"Done!\")\n",
    "        print(\"Words exist in W2V: \",count_exist)\n",
    "        print(\"Words don't exist in W2V: \",count_doesnt_exist)\n",
    "    print(len(x_embedding))\n",
    "    return x_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross Validation\n",
    "# from sklearn.model_selection import cross_val_score, KFold\n",
    "def get_cross_val_score(clf_pipeline,word2vec=False):\n",
    "    Result = {'scores_PM':[],'scores_RM':[]}\n",
    "    for i in range(6):\n",
    "        print(\"Analzying Fold:\",i+1)\n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "        file_train_name = 'datafinal/6fold/Fold_train'+str(i+1)+'.csv'\n",
    "        file_test_name = 'datafinal/6fold/Fold_test'+str(i+1)+'.csv'\n",
    "        print(\"Loading for train:\",file_train_name)\n",
    "        #load train fold\n",
    "        with open(file_train_name, newline='', encoding='utf-8') as csvfile:\n",
    "            #next(csvfile, None) # skip first header line\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            for row in csvreader:\n",
    "                if( \"2\" in row[3] ):\n",
    "                    X_train.append(row[2].lower())\n",
    "                    if(row[5] == 'Inquiry'):\n",
    "                        Y_train.append('Problem Discovery')\n",
    "                    else:\n",
    "                        Y_train.append(row[5])\n",
    "        #load test fold\n",
    "        print(\"Loading for test:\",file_test_name)\n",
    "        with open(file_test_name, newline='', encoding='utf-8') as csvfile:\n",
    "            #next(csvfile, None) # skip first header line\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            for row in csvreader:\n",
    "                if( \"2\" in row[3] ):\n",
    "                    X_test.append(row[2].lower())\n",
    "                    if(row[5] == 'Inquiry'):\n",
    "                        Y_test.append('Problem Discovery')\n",
    "                    else:\n",
    "                        Y_test.append(row[5])\n",
    "        if(word2vec):\n",
    "#           w2vname = 'NotIncludeDataset'\n",
    "            w2vname = 'AmazonW2VtrainedLowerNew'\n",
    "            X_train = generate_embedding(w2vname,X_train,stopword=False,verbose=False)\n",
    "            X_test = generate_embedding(w2vname,X_test,stopword=False,verbose=False)\n",
    "        Y_train = le.transform(Y_train)\n",
    "        Y_test = le.transform(Y_test)\n",
    "        clf_pipeline.fit(X_train,Y_train)\n",
    "        y_predict = clf_pipeline.predict(X_test)\n",
    "        if(word2vec):\n",
    "            metrics.writemulticlass(Y_test,y_predict,'Results/6foldSoftware/w2v/Fold_w2v_'+str(i+1))\n",
    "        else:\n",
    "            metrics.writemulticlass(Y_test,y_predict,'Results/6foldSoftware/tfidf/Fold_'+str(i+1))\n",
    "        Result['scores_PM'].append(precision_score(Y_test, y_predict,average='macro'))\n",
    "        Result['scores_RM'].append(recall_score(Y_test, y_predict,average='macro'))\n",
    "    print(\"==\"*30)\n",
    "    print(\"Result:\"+str(Result))\n",
    "    print(\"==\"*30)\n",
    "    print(\"Precision Micro: %0.3f (+/- %0.3f)\" % (mean(Result['scores_PM']), stdev(Result['scores_PM']) * 2))\n",
    "    print(\"Recall Micro: %0.3f (+/- %0.3f)\" % (mean(Result['scores_RM']), stdev(Result['scores_RM']) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with TF-idf using LinearSVC\n",
    "Best for jaccard and F1 -> C=0.1, class_weight=\"balanced\"\n",
    "<br>\n",
    "Best for subset accuracy (exact matching) -> C = 1\n",
    "<br>\n",
    "Including stopwords worsen the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for normal text classification with tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words=None)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf',LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analzying Fold: 1\n",
      "Loading for train: datafinal/6fold/Fold_train1.csv\n",
      "Loading for test: datafinal/6fold/Fold_test1.csv\n",
      "Analzying Fold: 2\n",
      "Loading for train: datafinal/6fold/Fold_train2.csv\n",
      "Loading for test: datafinal/6fold/Fold_test2.csv\n",
      "Analzying Fold: 3\n",
      "Loading for train: datafinal/6fold/Fold_train3.csv\n",
      "Loading for test: datafinal/6fold/Fold_test3.csv\n",
      "Analzying Fold: 4\n",
      "Loading for train: datafinal/6fold/Fold_train4.csv\n",
      "Loading for test: datafinal/6fold/Fold_test4.csv\n",
      "Analzying Fold: 5\n",
      "Loading for train: datafinal/6fold/Fold_train5.csv\n",
      "Loading for test: datafinal/6fold/Fold_test5.csv\n",
      "Analzying Fold: 6\n",
      "Loading for train: datafinal/6fold/Fold_train6.csv\n",
      "Loading for test: datafinal/6fold/Fold_test6.csv\n",
      "============================================================\n",
      "Result:{'scores_PM': [0.66407187532285883, 0.70823220064724923, 0.5703474372205023, 0.64842832393704486, 0.70272653576347144, 0.69203129250480533], 'scores_RM': [0.6072545947108513, 0.67878895968783615, 0.57175200278648564, 0.6764180389624368, 0.61504476788160023, 0.59775879812353105]}\n",
      "============================================================\n",
      "Precision Micro: 0.664 (+/- 0.103)\n",
      "Recall Micro: 0.625 (+/- 0.087)\n"
     ]
    }
   ],
   "source": [
    "get_cross_val_score(text_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('clf', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analzying Fold: 1\n",
      "Loading for train: datafinal/6fold/Fold_train1.csv\n",
      "Loading for test: datafinal/6fold/Fold_test1.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1521\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "402\n",
      "Analzying Fold: 2\n",
      "Loading for train: datafinal/6fold/Fold_train2.csv\n",
      "Loading for test: datafinal/6fold/Fold_test2.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1748\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "175\n",
      "Analzying Fold: 3\n",
      "Loading for train: datafinal/6fold/Fold_train3.csv\n",
      "Loading for test: datafinal/6fold/Fold_test3.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1829\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "94\n",
      "Analzying Fold: 4\n",
      "Loading for train: datafinal/6fold/Fold_train4.csv\n",
      "Loading for test: datafinal/6fold/Fold_test4.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1655\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "268\n",
      "Analzying Fold: 5\n",
      "Loading for train: datafinal/6fold/Fold_train5.csv\n",
      "Loading for test: datafinal/6fold/Fold_test5.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1485\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "438\n",
      "Analzying Fold: 6\n",
      "Loading for train: datafinal/6fold/Fold_train6.csv\n",
      "Loading for test: datafinal/6fold/Fold_test6.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1377\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "546\n",
      "============================================================\n",
      "Result:{'scores_PM': [0.79104477611940294, 0.80571428571428572, 0.78723404255319152, 0.69776119402985071, 0.70776255707762559, 0.7142857142857143], 'scores_RM': [0.79104477611940294, 0.80571428571428572, 0.78723404255319152, 0.69776119402985071, 0.70776255707762559, 0.7142857142857143]}\n",
      "============================================================\n",
      "Precision Micro: 0.751 (+/- 0.098)\n",
      "Recall Micro: 0.751 (+/- 0.098)\n"
     ]
    }
   ],
   "source": [
    "get_cross_val_score(pipeline,word2vec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
