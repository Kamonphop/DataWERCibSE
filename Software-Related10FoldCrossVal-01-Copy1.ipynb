{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import gensim.models.word2vec as w2v\n",
    "import string\n",
    "\n",
    "#nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import map_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#sklearn\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#statistics\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "#metrics\n",
    "import mlc_metrics as metrics\n",
    "\n",
    "#Define Scorer for Cross-Validation\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    return jaccard_similarity_score(ground_truth,predictions)\n",
    "def my_hammloss(ground_truth, predictions):\n",
    "    return metrics.mlc_hamming_loss(ground_truth, predictions)\n",
    "def my_custom_f1(ground_truth, predictions):\n",
    "    return metrics.mlc_f1score(ground_truth,predictions)\n",
    "\n",
    "jaccard  = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "hammloss = make_scorer(my_hammloss)\n",
    "f1score = make_scorer(my_custom_f1,greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allfold_Y = []\n",
    "with open('datafinal/fulldata.csv', encoding='utf-8') as csvfile:\n",
    "    next(csvfile, None) # skip first header line\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "    n = 0\n",
    "    for row in csvreader:\n",
    "        if( \"2\" in row[3] ):\n",
    "            if(row[5] == 'Inquiry'):\n",
    "                allfold_Y.append('Problem Discovery')\n",
    "            else:\n",
    "                allfold_Y.append(row[5])\n",
    "print(len(allfold_Y))\n",
    "le = LabelEncoder()\n",
    "le.fit(allfold_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "additional_stopwords = [\"im\",\"weve\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(additional_stopwords)#add another stop words\n",
    "stop_words = stop_words.union(list(string.punctuation))#add punctuation\n",
    "def stopword_and_punc_removal(x):\n",
    "    return \" \".join(filter(lambda word: word not in stop_words, re.sub(\"[^a-zA-z]\",\" \",x).split()))\n",
    "#     return x\n",
    "def tag_pos(x):\n",
    "#     token = TweetTokenizer().tokenize(x)\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",x)\n",
    "    token = clean.split()\n",
    "    pos = pos_tag(token)\n",
    "    simplified_tags = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in pos]\n",
    "    return simplified_tags\n",
    "\n",
    "def tokenize(x):\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",x)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def generate_embedding(w2vname,X,stopword=False,verbose=False):\n",
    "    print(\"Loading Amazon pre-trained Word2Vec:\",w2vname)\n",
    "    # Amazon_w2v = w2v.Word2Vec.load(os.path.join(\"AmazonW2VtrainedLowerNew\",\"AmazonW2VtrainedLowerNew.w2v\"))\n",
    "    path = 'word2vec/'+w2vname\n",
    "    filename = w2vname+\".w2v\"\n",
    "    w2vmodel = w2v.Word2Vec.load(os.path.join(path,filename))\n",
    "    print(\"Word2Vec Loaded!\")\n",
    "    pos_filter = ['NOUN','VERB','ADV','ADJ']\n",
    "    x_embedding = []\n",
    "    N = len(X)\n",
    "    percent_done = 0\n",
    "    count_exist = 0\n",
    "    count_doesnt_exist = 0\n",
    "    for i in range(N):\n",
    "        count = 1\n",
    "        sent_vector = np.zeros(300) #initialize a dummy vector\n",
    "        if(stopword):\n",
    "            sent_tagged_pos = tag_pos(stopword_and_punc_removal(X[i]))\n",
    "        else:\n",
    "            sent_tagged_pos = tag_pos(X[i])\n",
    "#         print(sent_tagged_pos)\n",
    "        for word in sent_tagged_pos:\n",
    "            if(word[1] in pos_filter):\n",
    "                if(word[0] in w2vmodel.wv.vocab):\n",
    "                    sent_vector += w2vmodel[word[0]]\n",
    "                    count+=1\n",
    "                    count_exist +=1\n",
    "                else:\n",
    "                    #try to make it lowercase- if word2vec not lower\n",
    "                    if(word[0].lower() in w2vmodel.wv.vocab):\n",
    "                        sent_vector += w2vmodel[word[0].lower()]\n",
    "                        count+=1\n",
    "                        count_exist+=1\n",
    "                    else:\n",
    "                        count_doesnt_exist+=1\n",
    "                        \n",
    "        #averaging vector\n",
    "        sent_vector /= count\n",
    "    \n",
    "        if(np.isnan(np.min(sent_vector))):\n",
    "            print(\"YES\")\n",
    "            continue\n",
    "        \n",
    "        x_embedding.append(sent_vector)\n",
    "        \n",
    "        if(verbose):\n",
    "            percent_done += 1\n",
    "            if(percent_done % int(0.25*N)) == 0:\n",
    "                print(\"Progress: \",percent_done,\" / \",N)\n",
    "    if(verbose):\n",
    "        print(\"Done!\")\n",
    "        print(\"Words exist in W2V: \",count_exist)\n",
    "        print(\"Words don't exist in W2V: \",count_doesnt_exist)\n",
    "    print(len(x_embedding))\n",
    "    return x_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross Validation\n",
    "# from sklearn.model_selection import cross_val_score, KFold\n",
    "def get_cross_val_score(clf_pipeline,word2vec=False):\n",
    "    Result = {'scores_PM':[],'scores_RM':[]}\n",
    "    for i in range(10):\n",
    "        print(\"Analzying Fold:\",i+1)\n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "        file_train_name = 'datafinal/10fold/Fold_train'+str(i+1)+'.csv'\n",
    "        file_test_name = 'datafinal/10fold/Fold_test'+str(i+1)+'.csv'\n",
    "        print(\"Loading for train:\",file_train_name)\n",
    "        #load train fold\n",
    "        with open(file_train_name, newline='', encoding='utf-8') as csvfile:\n",
    "            #next(csvfile, None) # skip first header line\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            for row in csvreader:\n",
    "                if( \"2\" in row[3] ):\n",
    "                    X_train.append(row[2].lower())\n",
    "                    if(row[5] == 'Inquiry'):\n",
    "                        Y_train.append('Problem Discovery')\n",
    "                    else:\n",
    "                        Y_train.append(row[5])\n",
    "        #load test fold\n",
    "        print(\"Loading for test:\",file_test_name)\n",
    "        with open(file_test_name, newline='', encoding='utf-8') as csvfile:\n",
    "            #next(csvfile, None) # skip first header line\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            for row in csvreader:\n",
    "                if( \"2\" in row[3] ):\n",
    "                    X_test.append(row[2].lower())\n",
    "                    if(row[5] == 'Inquiry'):\n",
    "                        Y_test.append('Problem Discovery')\n",
    "                    else:\n",
    "                        Y_test.append(row[5])\n",
    "        if(word2vec):\n",
    "#           w2vname = 'NotIncludeDataset'\n",
    "            w2vname = 'AmazonW2VtrainedLowerNew'\n",
    "            X_train = generate_embedding(w2vname,X_train,stopword=False,verbose=False)\n",
    "            X_test = generate_embedding(w2vname,X_test,stopword=False,verbose=False)\n",
    "        Y_train = le.transform(Y_train)\n",
    "        Y_test = le.transform(Y_test)\n",
    "        clf_pipeline.fit(X_train,Y_train)\n",
    "        y_predict = clf_pipeline.predict(X_test)\n",
    "        if(word2vec):\n",
    "            metrics.writemulticlass(Y_test,y_predict,'Results/10foldSoftware/w2v/Fold_w2v_'+str(i+1))\n",
    "        else:\n",
    "            metrics.writemulticlass(Y_test,y_predict,'Results/10foldSoftware/tfidf/Fold_'+str(i+1))\n",
    "        Result['scores_PM'].append(precision_score(Y_test, y_predict,average='macro'))\n",
    "        Result['scores_RM'].append(recall_score(Y_test, y_predict,average='macro'))\n",
    "    print(\"==\"*30)\n",
    "    print(\"Result:\"+str(Result))\n",
    "    print(\"==\"*30)\n",
    "    print(\"Precision Micro: %0.3f (+/- %0.3f)\" % (mean(Result['scores_PM']), stdev(Result['scores_PM']) * 2))\n",
    "    print(\"Recall Micro: %0.3f (+/- %0.3f)\" % (mean(Result['scores_RM']), stdev(Result['scores_RM']) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with TF-idf using LinearSVC\n",
    "Best for jaccard and F1 -> C=0.1, class_weight=\"balanced\"\n",
    "<br>\n",
    "Best for subset accuracy (exact matching) -> C = 1\n",
    "<br>\n",
    "Including stopwords worsen the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for normal text classification with tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words=None)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf',LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analzying Fold: 1\n",
      "Loading for train: datafinal/10fold/Fold_train1.csv\n",
      "Loading for test: datafinal/10fold/Fold_test1.csv\n",
      "Analzying Fold: 2\n",
      "Loading for train: datafinal/10fold/Fold_train2.csv\n",
      "Loading for test: datafinal/10fold/Fold_test2.csv\n",
      "Analzying Fold: 3\n",
      "Loading for train: datafinal/10fold/Fold_train3.csv\n",
      "Loading for test: datafinal/10fold/Fold_test3.csv\n",
      "Analzying Fold: 4\n",
      "Loading for train: datafinal/10fold/Fold_train4.csv\n",
      "Loading for test: datafinal/10fold/Fold_test4.csv\n",
      "Analzying Fold: 5\n",
      "Loading for train: datafinal/10fold/Fold_train5.csv\n",
      "Loading for test: datafinal/10fold/Fold_test5.csv\n",
      "Analzying Fold: 6\n",
      "Loading for train: datafinal/10fold/Fold_train6.csv\n",
      "Loading for test: datafinal/10fold/Fold_test6.csv\n",
      "Analzying Fold: 7\n",
      "Loading for train: datafinal/10fold/Fold_train7.csv\n",
      "Loading for test: datafinal/10fold/Fold_test7.csv\n",
      "Analzying Fold: 8\n",
      "Loading for train: datafinal/10fold/Fold_train8.csv\n",
      "Loading for test: datafinal/10fold/Fold_test8.csv\n",
      "Analzying Fold: 9\n",
      "Loading for train: datafinal/10fold/Fold_train9.csv\n",
      "Loading for test: datafinal/10fold/Fold_test9.csv\n",
      "Analzying Fold: 10\n",
      "Loading for train: datafinal/10fold/Fold_train10.csv\n",
      "Loading for test: datafinal/10fold/Fold_test10.csv\n",
      "============================================================\n",
      "Result:{'scores_PM': [0.79301712779973654, 0.65440115440115443, 0.68561827956989252, 0.7530511060259345, 0.73376166730597114, 0.7691645133505598, 0.76832715068009183, 0.77074145712443587, 0.79150326797385639, 0.76071066724562175], 'scores_RM': [0.71565960235629422, 0.59197260125622708, 0.68619528619528625, 0.67609490435231123, 0.64472573839662439, 0.66074074074074074, 0.65410486664648715, 0.67776177456628572, 0.70112035280574603, 0.67818986568986572]}\n",
      "============================================================\n",
      "Precision Micro: 0.748 (+/- 0.090)\n",
      "Recall Micro: 0.669 (+/- 0.068)\n"
     ]
    }
   ],
   "source": [
    "get_cross_val_score(text_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('clf', LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analzying Fold: 1\n",
      "Loading for train: datafinal/10fold/Fold_train1.csv\n",
      "Loading for test: datafinal/10fold/Fold_test1.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1730\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "193\n",
      "Analzying Fold: 2\n",
      "Loading for train: datafinal/10fold/Fold_train2.csv\n",
      "Loading for test: datafinal/10fold/Fold_test2.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1731\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "192\n",
      "Analzying Fold: 3\n",
      "Loading for train: datafinal/10fold/Fold_train3.csv\n",
      "Loading for test: datafinal/10fold/Fold_test3.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1719\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "204\n",
      "Analzying Fold: 4\n",
      "Loading for train: datafinal/10fold/Fold_train4.csv\n",
      "Loading for test: datafinal/10fold/Fold_test4.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1743\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "180\n",
      "Analzying Fold: 5\n",
      "Loading for train: datafinal/10fold/Fold_train5.csv\n",
      "Loading for test: datafinal/10fold/Fold_test5.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1744\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "179\n",
      "Analzying Fold: 6\n",
      "Loading for train: datafinal/10fold/Fold_train6.csv\n",
      "Loading for test: datafinal/10fold/Fold_test6.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1742\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "181\n",
      "Analzying Fold: 7\n",
      "Loading for train: datafinal/10fold/Fold_train7.csv\n",
      "Loading for test: datafinal/10fold/Fold_test7.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1721\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "202\n",
      "Analzying Fold: 8\n",
      "Loading for train: datafinal/10fold/Fold_train8.csv\n",
      "Loading for test: datafinal/10fold/Fold_test8.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1718\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "205\n",
      "Analzying Fold: 9\n",
      "Loading for train: datafinal/10fold/Fold_train9.csv\n",
      "Loading for test: datafinal/10fold/Fold_test9.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1736\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "187\n",
      "Analzying Fold: 10\n",
      "Loading for train: datafinal/10fold/Fold_train10.csv\n",
      "Loading for test: datafinal/10fold/Fold_test10.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1723\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "200\n",
      "============================================================\n",
      "Result:{'scores_PM': [0.72811523402421063, 0.7356516924262122, 0.70481698707505158, 0.77693763559987072, 0.74096385542168675, 0.80307486631016045, 0.72892158604921153, 0.71333333333333326, 0.75005418499394405, 0.71403508771929836], 'scores_RM': [0.69829984072914042, 0.71260017327268788, 0.67441077441077446, 0.67503552339487349, 0.66566455696202531, 0.66074074074074074, 0.63871119631386009, 0.6440695895583114, 0.6930460301246818, 0.66777319902319909]}\n",
      "============================================================\n",
      "Precision Micro: 0.740 (+/- 0.061)\n",
      "Recall Micro: 0.673 (+/- 0.046)\n"
     ]
    }
   ],
   "source": [
    "get_cross_val_score(pipeline,word2vec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
