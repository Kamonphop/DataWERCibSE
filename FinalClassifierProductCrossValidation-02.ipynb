{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import gensim.models.word2vec as w2v\n",
    "import string\n",
    "\n",
    "#nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import map_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#sklearn\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#stats\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "#metrics\n",
    "import mlc_metrics as metrics\n",
    "\n",
    "#Define Scorer for Cross-Validation\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    return jaccard_similarity_score(ground_truth,predictions)\n",
    "def my_hammloss(ground_truth, predictions):\n",
    "    return metrics.mlc_hamming_loss(ground_truth, predictions)\n",
    "def my_custom_f1(ground_truth, predictions):\n",
    "    return metrics.mlc_f1score(ground_truth,predictions)\n",
    "\n",
    "jaccard  = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "hammloss = make_scorer(my_hammloss)\n",
    "f1score = make_scorer(my_custom_f1,greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "def collapse_4andabove(stringlabels):\n",
    "    arraylabels = []\n",
    "    for i in stringlabels.strip().split(','):\n",
    "        if len(i) != 0:\n",
    "            label = int(i.strip())\n",
    "            if label == 1 or label == 2 or label == 3:\n",
    "                arraylabels.append(label)\n",
    "            else:\n",
    "                label = 4\n",
    "                if label not in arraylabels:\n",
    "                    arraylabels.append(label)\n",
    "        else:\n",
    "            print(\"Found data not annotated\")\n",
    "    return arraylabels\n",
    "\n",
    "# Load a multi-label dataset\n",
    "with open('datafinal/fulldata.csv', encoding='utf-8') as csvfile:\n",
    "    next(csvfile, None)\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "    n = 0\n",
    "    for row in csvreader:\n",
    "        sentence = row[2].lower()\n",
    "        labels = collapse_4andabove(row[3])\n",
    "        X.append(sentence)\n",
    "        Y.append(labels)\n",
    "        n += 1\n",
    "        \n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7198, 7198)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X),n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "additional_stopwords = [\"im\",\"weve\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(additional_stopwords)#add another stop words\n",
    "stop_words = stop_words.union(list(string.punctuation))#add punctuation\n",
    "\n",
    "def stopword_and_punc_removal(x):\n",
    "    return \" \".join(filter(lambda word: word not in stop_words, re.sub(\"[^a-zA-z]\",\" \",x).split()))\n",
    "#     return x\n",
    "\n",
    "def tag_pos(x):\n",
    "#     token = TweetTokenizer().tokenize(x)\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",x)\n",
    "    token = clean.split()\n",
    "    pos = pos_tag(token)\n",
    "    simplified_tags = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in pos]\n",
    "    return simplified_tags\n",
    "\n",
    "def tokenize(x):\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",x)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# print(\"Loading Amazon pre-trained Word2Vec\")\n",
    "# # Amazon_w2v = w2v.Word2Vec.load(os.path.join(\"AmazonW2VtrainedLowerNew\",\"AmazonW2VtrainedLowerNew.w2v\"))\n",
    "# Amazon_w2v = w2v.Word2Vec.load(os.path.join(\"word2vec/NotIncludeDataset\",\"NotIncludeDataset.w2v\"))\n",
    "# print(\"Word2Vec Loaded!\")\n",
    "\n",
    "def generate_embedding(w2vname,X,stopword=False,verbose=False):\n",
    "    print(\"Loading Amazon pre-trained Word2Vec:\",w2vname)\n",
    "    # Amazon_w2v = w2v.Word2Vec.load(os.path.join(\"AmazonW2VtrainedLowerNew\",\"AmazonW2VtrainedLowerNew.w2v\"))\n",
    "    path = 'word2vec/'+w2vname\n",
    "    filename = w2vname+\".w2v\"\n",
    "    w2vmodel = w2v.Word2Vec.load(os.path.join(path,filename))\n",
    "    print(\"Word2Vec Loaded!\")\n",
    "    pos_filter = ['NOUN','VERB','ADV','ADJ']\n",
    "    x_embedding = []\n",
    "    N = len(X)\n",
    "    percent_done = 0\n",
    "    count_exist = 0\n",
    "    count_doesnt_exist = 0\n",
    "    for i in range(N):\n",
    "        count = 1\n",
    "        sent_vector = np.zeros(300) #initialize a dummy vector\n",
    "        if(stopword):\n",
    "            sent_tagged_pos = tag_pos(stopword_and_punc_removal(X[i]))\n",
    "        else:\n",
    "            sent_tagged_pos = tag_pos(X[i])\n",
    "#         print(sent_tagged_pos)\n",
    "        for word in sent_tagged_pos:\n",
    "            if(word[1] in pos_filter):\n",
    "                if(word[0] in w2vmodel.wv.vocab):\n",
    "                    sent_vector += w2vmodel[word[0]]\n",
    "                    count+=1\n",
    "                    count_exist +=1\n",
    "                else:\n",
    "                    #try to make it lowercase- if word2vec not lower\n",
    "                    if(word[0].lower() in w2vmodel.wv.vocab):\n",
    "                        sent_vector += w2vmodel[word[0].lower()]\n",
    "                        count+=1\n",
    "                        count_exist+=1\n",
    "                    else:\n",
    "                        count_doesnt_exist+=1\n",
    "                        \n",
    "        #averaging vector\n",
    "        sent_vector /= count\n",
    "    \n",
    "        if(np.isnan(np.min(sent_vector))):\n",
    "            print(\"YES\")\n",
    "            continue\n",
    "        \n",
    "        x_embedding.append(sent_vector)\n",
    "        \n",
    "        if(verbose):\n",
    "            percent_done += 1\n",
    "            if(percent_done % int(0.25*N)) == 0:\n",
    "                print(\"Progress: \",percent_done,\" / \",N)\n",
    "    if(verbose):\n",
    "        print(\"Done!\")\n",
    "        print(\"Words exist in W2V: \",count_exist)\n",
    "        print(\"Words don't exist in W2V: \",count_doesnt_exist)\n",
    "    print(len(x_embedding))\n",
    "    return x_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cross_val_score(clf_pipeline,word2vec=False):\n",
    "    Result = {'scores_HL':[],'scores_Acc':[],'scores_EMR':[],'scores_F1':[],'scores_PM':[],'scores_RM':[]}\n",
    "    for i in range(6):\n",
    "        print(\"Analzying Fold:\",i+1)\n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "        file_train_name = 'datafinal/6fold/Fold_train'+str(i+1)+'.csv'\n",
    "        file_test_name = 'datafinal/6fold/Fold_test'+str(i+1)+'.csv'\n",
    "        print(\"Loading for train:\",file_train_name)\n",
    "        #load train fold\n",
    "        with open(file_train_name, newline='', encoding='utf-8') as csvfile:\n",
    "            #next(csvfile, None) # skip first header line\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            for row in csvreader:\n",
    "                X_train.append(row[2].lower())\n",
    "                Y_train.append(collapse_4andabove(row[3]))\n",
    "        #load test fold\n",
    "        print(\"Loading for test:\",file_test_name)\n",
    "        with open(file_test_name, newline='', encoding='utf-8') as csvfile:\n",
    "            #next(csvfile, None) # skip first header line\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            for row in csvreader:\n",
    "                X_test.append(row[2].lower())\n",
    "                Y_test.append(collapse_4andabove(row[3]))\n",
    "        if(word2vec):\n",
    "#             w2vname_fold = 'NotIncludeDataset'\n",
    "            w2vname = 'AmazonW2VtrainedLowerNew'\n",
    "            X_train = generate_embedding(w2vname,X_train,stopword=False,verbose=False)\n",
    "            X_test = generate_embedding(w2vname,X_test,stopword=False,verbose=False)\n",
    "            \n",
    "        Y_train = mlb.transform(Y_train)\n",
    "        Y_test = mlb.transform(Y_test)\n",
    "        clf_pipeline.fit(X_train,Y_train)\n",
    "        y_predict = clf_pipeline.predict(X_test)\n",
    "        \n",
    "        #specify the result folder\n",
    "        if(word2vec):\n",
    "            metrics.writeall(Y_test,y_predict,'Results/6foldTop/w2v/Fold_'+str(i+1))\n",
    "        else:\n",
    "            metrics.writeall(Y_test,y_predict,'Results/6foldTop/tfidf/Fold_'+str(i+1))\n",
    "        #------------------------------\n",
    "        Result['scores_HL'].append(metrics.mlc_hamming_loss(Y_test, y_predict))\n",
    "        Result['scores_Acc'].append(metrics.mlc_accuracy_score(Y_test, y_predict))\n",
    "        Result['scores_EMR'].append(accuracy_score(Y_test, y_predict))\n",
    "        Result['scores_F1'].append(metrics.mlc_f1score(Y_test, y_predict))\n",
    "        Result['scores_PM'].append(precision_score(Y_test, y_predict,average='micro'))\n",
    "        Result['scores_RM'].append(recall_score(Y_test, y_predict,average='micro'))\n",
    "    print(\"==\"*30)\n",
    "    print(\"Result:\"+str(Result))\n",
    "    print(\"==\"*30)\n",
    "    print(\"Hamm Loss: %0.3f (+/- %0.3f)\" % (mean(Result['scores_HL']), stdev(Result['scores_HL']) * 2))\n",
    "    print(\"Accuracy: %0.3f (+/- %0.3f)\" % (mean(Result['scores_Acc']), stdev(Result['scores_Acc']) * 2))\n",
    "    print(\"ExactMatchRatio: %0.3f (+/- %0.3f)\" % (mean(Result['scores_EMR']), stdev(Result['scores_EMR']) * 2))\n",
    "    print(\"F1: %0.3f (+/- %0.3f)\" % (mean(Result['scores_F1']), stdev(Result['scores_F1']) * 2))\n",
    "    print(\"Precision Micro: %0.3f (+/- %0.3f)\" % (mean(Result['scores_PM']), stdev(Result['scores_PM']) * 2))\n",
    "    print(\"Recall Micro: %0.3f (+/- %0.3f)\" % (mean(Result['scores_RM']), stdev(Result['scores_RM']) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with TF-idf using LinearSVC\n",
    "Best for jaccard and F1 -> C=0.1, class_weight=\"balanced\"\n",
    "<br>\n",
    "Best for subset accuracy (exact matching) -> C = 1\n",
    "<br>\n",
    "Including stopwords worsen the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for normal text classification with tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,stop_words=None)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', OneVsRestClassifier(LinearSVC(C=1)))\n",
    "#                      ('clf', OneVsRestClassifier(LinearSVC(C=0.1,class_weight='balanced')))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analzying Fold: 1\n",
      "Loading for train: datafinal/6fold/Fold_train1.csv\n",
      "Loading for test: datafinal/6fold/Fold_test1.csv\n",
      "Analzying Fold: 2\n",
      "Loading for train: datafinal/6fold/Fold_train2.csv\n",
      "Loading for test: datafinal/6fold/Fold_test2.csv\n",
      "Analzying Fold: 3\n",
      "Loading for train: datafinal/6fold/Fold_train3.csv\n",
      "Loading for test: datafinal/6fold/Fold_test3.csv\n",
      "Analzying Fold: 4\n",
      "Loading for train: datafinal/6fold/Fold_train4.csv\n",
      "Loading for test: datafinal/6fold/Fold_test4.csv\n",
      "Analzying Fold: 5\n",
      "Loading for train: datafinal/6fold/Fold_train5.csv\n",
      "Loading for test: datafinal/6fold/Fold_test5.csv\n",
      "Analzying Fold: 6\n",
      "Loading for train: datafinal/6fold/Fold_train6.csv\n",
      "Loading for test: datafinal/6fold/Fold_test6.csv\n",
      "============================================================\n",
      "Result:{'scores_HL': [0.22593669250645995, 0.22549019607843138, 0.23854166666666668, 0.20905923344947736, 0.22391107078039926, 0.22785027472527472], 'scores_Acc': [0.5325150732127479, 0.5175653594771246, 0.47199074074074066, 0.5542973286875725, 0.5096037507562006, 0.5232371794871795], 'scores_EMR': [0.40116279069767441, 0.38807189542483661, 0.36666666666666664, 0.40505226480836237, 0.38203266787658802, 0.38942307692307693], 'scores_F1': [0.59865470487472061, 0.58217496252779377, 0.52646185974939541, 0.62829695605521207, 0.57444665688647567, 0.59117444500831096], 'scores_PM': [0.68078575813382447, 0.66461063993831915, 0.64940828402366868, 0.6966292134831461, 0.66991150442477876, 0.66771752837326603], 'scores_RM': [0.55784708249496984, 0.56303069888961466, 0.49381327334083241, 0.59862068965517246, 0.55215171407731578, 0.56966110812264659]}\n",
      "============================================================\n",
      "Hamm Loss: 0.225 (+/- 0.019)\n",
      "Accuracy: 0.518 (+/- 0.055)\n",
      "ExactMatchRatio: 0.389 (+/- 0.028)\n",
      "F1: 0.584 (+/- 0.067)\n",
      "Precision Micro: 0.672 (+/- 0.032)\n",
      "Recall Micro: 0.556 (+/- 0.069)\n"
     ]
    }
   ],
   "source": [
    "get_cross_val_score(text_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('clf',OneVsRestClassifier(LinearSVC(C=1)))\n",
    "#     ('clf',OneVsRestClassifier(LinearSVC(C=0.1,class_weight='balanced')))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analzying Fold: 1\n",
      "Loading for train: datafinal/6fold/Fold_train1.csv\n",
      "Loading for test: datafinal/6fold/Fold_test1.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "5650\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1548\n",
      "Analzying Fold: 2\n",
      "Loading for train: datafinal/6fold/Fold_train2.csv\n",
      "Loading for test: datafinal/6fold/Fold_test2.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "5974\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1224\n",
      "Analzying Fold: 3\n",
      "Loading for train: datafinal/6fold/Fold_train3.csv\n",
      "Loading for test: datafinal/6fold/Fold_test3.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6478\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "720\n",
      "Analzying Fold: 4\n",
      "Loading for train: datafinal/6fold/Fold_train4.csv\n",
      "Loading for test: datafinal/6fold/Fold_test4.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6050\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1148\n",
      "Analzying Fold: 5\n",
      "Loading for train: datafinal/6fold/Fold_train5.csv\n",
      "Loading for test: datafinal/6fold/Fold_test5.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "6096\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1102\n",
      "Analzying Fold: 6\n",
      "Loading for train: datafinal/6fold/Fold_train6.csv\n",
      "Loading for test: datafinal/6fold/Fold_test6.csv\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "5742\n",
      "Loading Amazon pre-trained Word2Vec: AmazonW2VtrainedLowerNew\n",
      "Word2Vec Loaded!\n",
      "1456\n",
      "============================================================\n",
      "Result:{'scores_HL': [0.19864341085271317, 0.21180555555555555, 0.20590277777777777, 0.19403310104529617, 0.1867059891107078, 0.20449862637362637], 'scores_Acc': [0.5812876830318695, 0.5547385620915033, 0.5653935185185184, 0.585801393728223, 0.5960375075620085, 0.5681662087912088], 'scores_EMR': [0.44896640826873385, 0.42483660130718953, 0.44166666666666665, 0.45383275261324041, 0.46823956442831216, 0.43887362637362637], 'scores_F1': [0.64853432653497867, 0.61982424230676447, 0.62840208083970162, 0.65219661074513757, 0.66037418892631694, 0.63251499021199553], 'scores_PM': [0.73540372670807452, 0.6891271056661562, 0.69786096256684493, 0.73080099091659789, 0.7345890410958904, 0.71355498721227617], 'scores_RM': [0.59557344064386319, 0.58785107772697587, 0.58717660292463447, 0.6103448275862069, 0.62582056892778992, 0.60032275416890801]}\n",
      "============================================================\n",
      "Hamm Loss: 0.200 (+/- 0.018)\n",
      "Accuracy: 0.575 (+/- 0.030)\n",
      "ExactMatchRatio: 0.446 (+/- 0.029)\n",
      "F1: 0.640 (+/- 0.031)\n",
      "Precision Micro: 0.717 (+/- 0.040)\n",
      "Recall Micro: 0.601 (+/- 0.030)\n"
     ]
    }
   ],
   "source": [
    "get_cross_val_score(pipeline,word2vec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
